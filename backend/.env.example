# =============================================================================
# ADK.js Agent System - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values

# -----------------------------------------------------------------------------
# LLM Configuration (via LiteLLM OpenAI-compatible API)
# -----------------------------------------------------------------------------

# API key for your LLM provider (required)
AI_API_KEY=your-api-key-here

# Base URL for OpenAI-compatible API (LiteLLM proxy)
# Default: http://localhost:4000/v1
AI_BASE_URL=http://localhost:4000/v1

# Model identifier (use LiteLLM format: provider/model)
# Examples:
#   - gemini/gemini-2.0-flash (Google Gemini via LiteLLM)
#   - gemini/gemini-pro
#   - gpt-4o (OpenAI)
#   - anthropic/claude-3-5-sonnet-20241022 (Anthropic)
AI_MODEL=gemini/gemini-2.0-flash

# Or use a preset (overrides AI_MODEL if set)
# Available: gemini-2.0-flash, gemini-pro, gpt-4o, gpt-4o-mini, claude-3-5-sonnet
# AI_MODEL_PRESET=gemini-2.0-flash

# Global temperature (0.0 - 1.0)
AI_TEMPERATURE=0.7

# Max tokens for responses
AI_MAX_TOKENS=2000

# Enable reasoning mode (if model supports it)
AI_ENABLE_REASONING=false

# -----------------------------------------------------------------------------
# Agent-Specific Model Overrides (Optional)
# Use these to run different models for different agents
# -----------------------------------------------------------------------------

# QueryUnderstanding Agent (intent classification)
# AI_QUERY_UNDERSTANDING_MODEL=gemini/gemini-2.0-flash
# AI_QUERY_UNDERSTANDING_TEMPERATURE=0.3

# CodeGeneration Agent
# AI_CODE_GENERATION_MODEL=gemini/gemini-2.0-flash
# AI_CODE_GENERATION_MAX_TOKENS=4000

# Reasoning Agent
# AI_REASONING_MODEL=gemini/gemini-2.0-flash
# AI_REASONING_TEMPERATURE=0.7

# DataInsight Agent
# AI_DATA_INSIGHT_MODEL=gemini/gemini-2.0-flash

# -----------------------------------------------------------------------------
# PostgreSQL Database Configuration
# -----------------------------------------------------------------------------

DB_HOST=localhost
DB_PORT=5432
DB_NAME=agent_db
DB_USER=postgres
DB_PASSWORD=postgres

# -----------------------------------------------------------------------------
# MCP (Model Context Protocol) Configuration
# -----------------------------------------------------------------------------

# Enable MCP execution delegation (delegates to Docker container)
USE_MCP_EXECUTION=false

# MCP server URL (Docker container endpoint)
MCP_SERVER_URL=http://localhost:3001

# MCP connection timeout (milliseconds)
MCP_TIMEOUT=30000

# MCP API key (if authentication is required)
# MCP_API_KEY=your-mcp-api-key

# -----------------------------------------------------------------------------
# Server Configuration
# -----------------------------------------------------------------------------

# HTTP server port (use 3001 to avoid conflict with Next.js frontend on 3000)
PORT=3001

# Development mode
NODE_ENV=development

# -----------------------------------------------------------------------------
# LiteLLM Proxy Configuration (if running locally)
# -----------------------------------------------------------------------------

# To run LiteLLM proxy:
# pip install litellm
# litellm --model gemini/gemini-2.0-flash --port 4000
#
# Or with multiple models:
# litellm --config litellm_config.yaml

